{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrator-Workers Workflow\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Have you ever needed multiple perspectives on the same task, but couldn't predict in advance which perspectives would be most valuable? The orchestrator-workers pattern solves this by having a central LLM analyze each unique task and dynamically determine the best subtasks to delegate to specialized worker LLMs.\n",
    "\n",
    "Traditional approaches either require manual prompting multiple times or use hardcoded parallelization that generates the same variations regardless of context.\n",
    "\n",
    "With this approach, an orchestrator LLM analyzes the task, determines which variations would be most valuable for this specific case, then delegates to worker LLMs that generate each variation.\n",
    "\n",
    "### What You'll Build\n",
    "\n",
    "A system that takes a product description request and:\n",
    "\n",
    "1. Analyzes what types of marketing copy would be valuable\n",
    "2. Dynamically generates specialized task descriptions for workers\n",
    "3. Produces multiple content variations optimized for different audiences\n",
    "4. Returns coordinated results from all workers\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Python 3.9 or higher\n",
    "- Anthropic API key set as environment variable: `export ANTHROPIC_API_KEY='your-key'`\n",
    "- Basic understanding of prompt engineering\n",
    "- Familiarity with Python classes and type hints\n",
    "\n",
    "\n",
    "### When to use this workflow\n",
    "\n",
    "This workflow is well-suited for complex tasks where you can't predict the subtasks needed in advance. The key difference from simple parallelization is its flexibility—subtasks aren't pre-defined, but determined by the orchestrator based on the specific input.\n",
    "\n",
    "**Use this pattern when:**\n",
    "\n",
    "- Tasks require multiple distinct approaches or perspectives\n",
    "- The optimal subtasks depend on the specific input\n",
    "- You need to compare different strategies or styles\n",
    "\n",
    "**Don't use this pattern when:**\n",
    "\n",
    "- You have simple, single-output tasks (unnecessary complexity)\n",
    "- Latency is critical (multiple LLM calls add overhead)\n",
    "- Subtasks are predictable and can be pre-defined (use simpler parallelization)\n",
    "\n",
    "## How It Works\n",
    "\n",
    "The orchestrator-workers pattern operates in two phases:\n",
    "\n",
    "1. **Analysis & Planning Phase**: The orchestrator LLM receives the task and context, analyzes what approaches would be valuable, and generates structured subtask descriptions in XML format.\n",
    "\n",
    "2. **Execution Phase**: Each worker LLM receives:\n",
    "   - The original task for context\n",
    "   - Its specific subtask type and description\n",
    "   - Any additional context provided\n",
    "\n",
    "The orchestrator decides *at runtime* what subtasks to create, making this more adaptive than pre-defined parallel workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Installation\n",
    "```bash\n",
    "pip install anthropic\n",
    "```\n",
    "\n",
    "### Helper Functions\n",
    "This example uses helper functions from `util.py` for making LLM calls and parsing XML responses:\n",
    "\n",
    "- `llm_call(prompt, system_prompt=\"\", model=\"claude-sonnet-4-5\")`: Sends a prompt to Claude and returns the text response\n",
    "- `extract_xml(text, tag)`: Extracts content from XML tags using regex\n",
    "\n",
    "These utilities handle API authentication (reading `ANTHROPIC_API_KEY` from environment) and provide a simple interface for the orchestrator-workers pattern. You can view the complete implementation in [util.py](util.py).\n",
    "\n",
    "## Implementation\n",
    "\n",
    "The `FlexibleOrchestrator` class coordinates the two-phase workflow:\n",
    "\n",
    "**Key design decisions:**\n",
    "- Prompts are templates that accept runtime variables (`task`, `context`) for flexibility\n",
    "- XML is used for structured output parsing (reliable and language-model-friendly format)\n",
    "- Workers receive both the original task AND their specific instructions for better context\n",
    "- Error handling validates that workers return non-empty responses\n",
    "\n",
    "The implementation includes:\n",
    "- `parse_tasks()`: Parses the orchestrator's XML output into structured task dictionaries\n",
    "- `FlexibleOrchestrator.process()`: Main coordination logic that calls orchestrator, then workers\n",
    "- Response validation to catch and handle empty worker outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import extract_xml, llm_call\n",
    "\n",
    "# Model configuration\n",
    "MODEL = \"claude-sonnet-4-5\"  # Fast, capable model for both orchestrator and workers\n",
    "\n",
    "\n",
    "def parse_tasks(tasks_xml: str) -> list[dict]:\n",
    "    \"\"\"Parse XML tasks into a list of task dictionaries.\"\"\"\n",
    "    tasks = []\n",
    "    current_task = {}\n",
    "\n",
    "    for line in tasks_xml.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        if line.startswith(\"<task>\"):\n",
    "            current_task = {}\n",
    "        elif line.startswith(\"<type>\"):\n",
    "            current_task[\"type\"] = line[6:-7].strip()\n",
    "        elif line.startswith(\"<description>\"):\n",
    "            current_task[\"description\"] = line[12:-13].strip()\n",
    "        elif line.startswith(\"</task>\"):\n",
    "            if \"description\" in current_task:\n",
    "                if \"type\" not in current_task:\n",
    "                    current_task[\"type\"] = \"default\"\n",
    "                tasks.append(current_task)\n",
    "\n",
    "    return tasks\n",
    "\n",
    "\n",
    "class FlexibleOrchestrator:\n",
    "    \"\"\"Break down tasks and run them in parallel using worker LLMs.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        orchestrator_prompt: str,\n",
    "        worker_prompt: str,\n",
    "        model: str = MODEL,\n",
    "    ):\n",
    "        \"\"\"Initialize with prompt templates and model selection.\"\"\"\n",
    "        self.orchestrator_prompt = orchestrator_prompt\n",
    "        self.worker_prompt = worker_prompt\n",
    "        self.model = model\n",
    "\n",
    "    def _format_prompt(self, template: str, **kwargs) -> str:\n",
    "        \"\"\"Format a prompt template with variables.\"\"\"\n",
    "        try:\n",
    "            return template.format(**kwargs)\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Missing required prompt variable: {e}\") from e\n",
    "\n",
    "    def process(self, task: str, context: dict | None = None) -> dict:\n",
    "        \"\"\"Process task by breaking it down and running subtasks in parallel.\"\"\"\n",
    "        context = context or {}\n",
    "\n",
    "        # Step 1: Get orchestrator response\n",
    "        orchestrator_input = self._format_prompt(self.orchestrator_prompt, task=task, **context)\n",
    "        orchestrator_response = llm_call(orchestrator_input, model=self.model)\n",
    "\n",
    "        # Parse orchestrator response\n",
    "        analysis = extract_xml(orchestrator_response, \"analysis\")\n",
    "        tasks_xml = extract_xml(orchestrator_response, \"tasks\")\n",
    "        tasks = parse_tasks(tasks_xml)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ORCHESTRATOR ANALYSIS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\n{analysis}\\n\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"IDENTIFIED {len(tasks)} APPROACHES\")\n",
    "        print(\"=\" * 80)\n",
    "        for i, task_info in enumerate(tasks, 1):\n",
    "            print(f\"\\n{i}. {task_info['type'].upper()}\")\n",
    "            print(f\"   {task_info['description']}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"GENERATING CONTENT\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "        # Step 2: Process each task\n",
    "        worker_results = []\n",
    "        for i, task_info in enumerate(tasks, 1):\n",
    "            print(f\"[{i}/{len(tasks)}] Processing: {task_info['type']}...\")\n",
    "\n",
    "            worker_input = self._format_prompt(\n",
    "                self.worker_prompt,\n",
    "                original_task=task,\n",
    "                task_type=task_info[\"type\"],\n",
    "                task_description=task_info[\"description\"],\n",
    "                **context,\n",
    "            )\n",
    "\n",
    "            worker_response = llm_call(worker_input, model=self.model)\n",
    "            worker_content = extract_xml(worker_response, \"response\")\n",
    "\n",
    "            # Validate worker response - handle empty outputs\n",
    "            if not worker_content or not worker_content.strip():\n",
    "                print(f\"Warning: Worker '{task_info['type']}' returned no content\")\n",
    "                worker_content = f\"[Error: Worker '{task_info['type']}' failed to generate content]\"\n",
    "\n",
    "            worker_results.append(\n",
    "                {\n",
    "                    \"type\": task_info[\"type\"],\n",
    "                    \"description\": task_info[\"description\"],\n",
    "                    \"result\": worker_content,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"RESULTS\")\n",
    "        print(\"=\" * 80)\n",
    "        for i, result in enumerate(worker_results, 1):\n",
    "            print(f\"\\n{'-' * 80}\")\n",
    "            print(f\"Approach {i}: {result['type'].upper()}\")\n",
    "            print(f\"{'-' * 80}\")\n",
    "            print(f\"\\n{result['result']}\\n\")\n",
    "\n",
    "        return {\n",
    "            \"analysis\": analysis,\n",
    "            \"worker_results\": worker_results,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Use Case: Marketing Variation Generation\n",
    "\n",
    "Now let's see the orchestrator-workers pattern in action with a practical example: generating multiple styles of marketing copy for a product.\n",
    "\n",
    "**Why this example demonstrates the pattern well:**\n",
    "- Different products benefit from different marketing angles\n",
    "- The \"best\" variations depend on the specific product features and target audience\n",
    "- The orchestrator can adapt its strategy based on the input rather than using a fixed template\n",
    "\n",
    "**Prompt design notes:**\n",
    "- The orchestrator prompt asks for 2-3 approaches and provides XML structure guidance\n",
    "- The worker prompt gives workers full context (original task, their style, and guidelines)\n",
    "- Both prompts use clear XML formatting to ensure reliable parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORCHESTRATOR_PROMPT = \"\"\"\n",
    "Analyze this task and break it down into 2-3 distinct approaches:\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Return your response in this format:\n",
    "\n",
    "<analysis>\n",
    "Explain your understanding of the task and which variations would be valuable.\n",
    "Focus on how each approach serves different aspects of the task.\n",
    "</analysis>\n",
    "\n",
    "<tasks>\n",
    "    <task>\n",
    "    <type>formal</type>\n",
    "    <description>Write a precise, technical version that emphasizes specifications</description>\n",
    "    </task>\n",
    "    <task>\n",
    "    <type>conversational</type>\n",
    "    <description>Write an engaging, friendly version that connects with readers</description>\n",
    "    </task>\n",
    "</tasks>\n",
    "\"\"\"\n",
    "\n",
    "WORKER_PROMPT = \"\"\"\n",
    "Generate content based on:\n",
    "Task: {original_task}\n",
    "Style: {task_type}\n",
    "Guidelines: {task_description}\n",
    "\n",
    "Return your response in this format:\n",
    "\n",
    "<response>\n",
    "Your content here, maintaining the specified style and fully addressing requirements.\n",
    "</response>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "orchestrator = FlexibleOrchestrator(\n",
    "    orchestrator_prompt=ORCHESTRATOR_PROMPT,\n",
    "    worker_prompt=WORKER_PROMPT,\n",
    ")\n",
    "\n",
    "results = orchestrator.process(\n",
    "    task=\"Write a product description for a new eco-friendly water bottle\",\n",
    "    context={\n",
    "        \"target_audience\": \"environmentally conscious millennials\",\n",
    "        \"key_features\": [\"plastic-free\", \"insulated\", \"lifetime warranty\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've now implemented an orchestrator-workers pattern that dynamically adapts its task breakdown based on the specific input. This pattern generated multiple marketing copy variations—each tailored to different audiences and contexts—without requiring you to pre-define what those variations should be.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Pattern benefits:**\n",
    "- **Adaptability**: The orchestrator determines the best approach for each unique input\n",
    "- **Flexibility**: Easy to apply to different domains by changing the prompts\n",
    "- **Structured coordination**: XML-based communication ensures reliable parsing\n",
    "- **Error resilience**: Validation catches and handles worker failures\n",
    "\n",
    "**When this pattern excels:**\n",
    "- Content generation requiring multiple perspectives (marketing, documentation, creative writing)\n",
    "- Analysis tasks benefiting from different analytical lenses\n",
    "- Problem-solving where the decomposition strategy depends on the problem\n",
    "\n",
    "### Limitations & Considerations\n",
    "\n",
    "**Cost & Latency:**\n",
    "- Requires N+1 LLM calls (1 orchestrator + N workers)\n",
    "- Sequential processing in this implementation (workers run one at a time)\n",
    "- For better performance, consider parallelizing worker calls with `asyncio` or thread pools\n",
    "\n",
    "**When NOT to use this pattern:**\n",
    "- Simple tasks with single, clear outputs (the added complexity isn't justified)\n",
    "- Latency-critical applications (multiple API calls add overhead)\n",
    "- Tasks where subtasks are always the same (use pre-defined parallelization instead)\n",
    "\n",
    "**Failure modes to consider:**\n",
    "- Orchestrator might not break down tasks optimally (prompt engineering is critical)\n",
    "- Workers may return empty or malformed responses (we handle this with validation)\n",
    "- XML parsing can fail if models don't follow format exactly (consider using JSON as an alternative)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Enhance this implementation:**\n",
    "1. Add parallel worker execution using `asyncio` for better performance\n",
    "2. Implement retry logic for failed workers\n",
    "3. Add a synthesis phase where an LLM combines worker outputs\n",
    "4. Experiment with different orchestrator strategies (e.g., asking for more/fewer subtasks)\n",
    "\n",
    "**Adapt to your use case:**\n",
    "- Modify the orchestrator prompt to guide task decomposition for your domain\n",
    "- Adjust worker prompts to provide domain-specific instructions\n",
    "- Add context parameters relevant to your application\n",
    "- Consider using Claude Opus for the orchestrator and Claude Haiku for workers to optimize cost vs. quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
